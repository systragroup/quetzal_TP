{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "71ccff01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'training_folder': '../../scenarios/montreal_2', 'params': {'catchment_radius': {'bus': '500', 'subway': '800', 'rail': '1000'}}}\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import json\n",
    "\n",
    "catchment_radius={'bus':'500','subway':'800','rail':'1000'}\n",
    "\n",
    "params = {'catchment_radius':catchment_radius}\n",
    "\n",
    "default = {'training_folder': '../../scenarios/montreal_2', 'params':params} # Default execution parameters\n",
    "manual, argv = (True, default) if 'ipykernel' in sys.argv[0] else (False, dict(default, **json.loads(sys.argv[1])))\n",
    "print(argv)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ee5e2d8",
   "metadata": {},
   "source": [
    "io (read): 10 secs <br>\n",
    "mesh: 5-9 secs <br>\n",
    "acf_dist : 6-9 secs <br>\n",
    "metrics: 1.2 secs<br>\n",
    "<br>\n",
    "tot: 22-30 seconds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "99b5db61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numba threads 8\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "sys.path.insert(0, r'../../../quetzal') # Add path to quetzal\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from shapely.geometry import Point, LineString\n",
    "from syspy.spatial.spatial import add_geometry_coordinates, nearest\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from typing import Literal\n",
    "from numba import jit, njit\n",
    "import numba as nb\n",
    "#num_cores = 1\n",
    "print('numba threads',nb.config.NUMBA_NUM_THREADS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "dc057466",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from quetzal_cyclops\n",
    "def get_epsg(lat: float, lon: float) -> int:\n",
    "    '''\n",
    "    lat, lon or y, x\n",
    "    return EPSG in meter for a given (lat,lon)\n",
    "    lat is north south \n",
    "    lon is est west\n",
    "    '''\n",
    "    return int(32700 - round((45 + lat) / 90, 0) * 100 + round((183 + lon) / 6, 0))\n",
    "\n",
    "# from quetzal_cyclops\n",
    "def zones_nearest_node(zones,nodes,drop_duplicates=False):\n",
    "    # getting zones centroids\n",
    "    centroid = zones.copy()\n",
    "    centroid['geometry'] = centroid.centroid\n",
    "    # finding nearest node\n",
    "    neigh = nearest(centroid, nodes, n_neighbors=1).rename(columns={'ix_one': 'zone_index', 'ix_many': 'node_index'})\n",
    "    zone_node_dict = neigh.set_index('zone_index')['node_index'].to_dict()\n",
    "    centroid['node_index'] = centroid.index.map(zone_node_dict.get)\n",
    "    #print('max_distance found: ', neigh['distance'].max())\n",
    "    # check for duplicated nodes. if there is. drop the duplicated zones.\n",
    "    if drop_duplicates:\n",
    "        if len(centroid.drop_duplicates('node_index')) != len(centroid):\n",
    "            print('there is zones associates to the same road_node')\n",
    "            # duplicated = centroid[centroid['node_index'].duplicated()]['node_index'].values\n",
    "            print('dropping zones: ')\n",
    "            print(centroid[centroid['node_index'].duplicated()].index.values)\n",
    "            centroid = centroid.drop_duplicates('node_index')\n",
    "    return centroid\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "2595e6cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from quetzal.engine.pathfinder_utils import simple_routing, sparse_matrix, get_path\n",
    "from scipy.sparse import csr_matrix\n",
    "from scipy.sparse.csgraph import dijkstra\n",
    "\n",
    "@jit(nopython=True)\n",
    "def _unstack(mat):\n",
    "    # return non inf values in mat as [[row,col,val],[row,col,val]]. so, [o,d,val].\n",
    "    # pd.DataFrame of this gives us [origin, destination, value] as columns\n",
    "    row, col = np.where(np.isfinite(mat))\n",
    "    res = np.zeros((len(row),3))\n",
    "    for it in nb.prange(len(col)):\n",
    "        i=row[it]\n",
    "        j=col[it]\n",
    "        d=mat[i,j]\n",
    "        res[it]=[i,j,d]\n",
    "    return res\n",
    "\n",
    "def routing(origin, destination, links, weight_col='time', dijkstra_limit=np.inf):\n",
    "    mat, node_index = sparse_matrix(links[['a', 'b', weight_col]].values)\n",
    "    index_node = {v: k for k, v in node_index.items()}\n",
    "    # liste des origines pour le dijkstra\n",
    "    origin_sparse = [node_index[x] for x in origin]\n",
    "    origin_dict =  {i:val for i,val in enumerate(origin_sparse)}\n",
    "    # list des destinations \n",
    "    destination_sparse = [node_index[x] for x in destination]\n",
    "    destination_dict =  {i:val for i,val in enumerate(destination_sparse)}\n",
    "    # dijktra on the road network from node = incices to every other nodes.\n",
    "    # from b to a.\n",
    "    dist_matrix = dijkstra(\n",
    "        csgraph=mat,\n",
    "        directed=True,\n",
    "        indices=origin_sparse,\n",
    "        return_predecessors=False,\n",
    "        limit=dijkstra_limit\n",
    "    )\n",
    "    # remove non-used destination\n",
    "    dist_matrix = dist_matrix[:,destination_sparse]\n",
    "    # unstack amtrix\n",
    "    dist_matrix = pd.DataFrame(_unstack(dist_matrix),columns=['origin', 'destination', weight_col])\n",
    "    # rename origin and destination with original indexes.\n",
    "    dist_matrix['origin'] = dist_matrix['origin'].apply(lambda x: index_node.get(origin_dict.get(x)))\n",
    "    dist_matrix['destination'] = dist_matrix['destination'].apply(lambda x: index_node.get(destination_dict.get(x)))\n",
    "    return dist_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "bb087e97",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_catchment_dist(link: gpd.GeoDataFrame, catchment_radius: dict, default: float=500):\n",
    "    route_type = link['route_type'].unique()\n",
    "    if len(route_type)>1:\n",
    "        print('multiple route type for a single route_id.. using first one for catchment radius')\n",
    "    route_type = route_type[0]\n",
    "    return catchment_radius.get(route_type, default)\n",
    "\n",
    "\n",
    "def nearest_radius(one, many, radius=100,to_crs=None):\n",
    "    try:\n",
    "        # Assert df_many.index.is_unique\n",
    "        assert one.index.is_unique\n",
    "        assert many.index.is_unique\n",
    "    except AssertionError:\n",
    "        msg = 'Index of one or many should not contain duplicates'\n",
    "        print(msg)\n",
    "        warnings.warn(msg)\n",
    "    many = add_geometry_coordinates(many, columns=['x_geometry', 'y_geometry'], to_crs=to_crs)\n",
    "    one = add_geometry_coordinates(one, columns=['x_geometry', 'y_geometry'], to_crs=to_crs)\n",
    "    \n",
    "    x = many[['x_geometry', 'y_geometry']].values\n",
    "    # Fit Nearest neighbors model\n",
    "    #nbrs = NearestNeighbors(n_neighbors=n_neighbors, algorithm='ball_tree').fit(x)\n",
    "    nbrs = NearestNeighbors(radius=radius,algorithm='ball_tree').fit(x)\n",
    "\n",
    "    y = one[['x_geometry', 'y_geometry']].values\n",
    "\n",
    "    #distances, indices = nbrs.kneighbors(y,return_distance=True)\n",
    "    distances, indices = nbrs.radius_neighbors(y, radius = radius, return_distance=True)\n",
    "\n",
    "    indices = pd.DataFrame(indices)\n",
    "    indices = pd.DataFrame(indices.stack(), columns=['index_nn']).reset_index().rename(\n",
    "        columns={'level_0': 'ix_one', 'level_1': 'rank'}\n",
    "    )\n",
    "    indices['distances'] = distances\n",
    "    return indices\n",
    "\n",
    "def create_mesh(zones: gpd.GeoDataFrame ,step: float = 0.01) -> gpd.GeoDataFrame:\n",
    "    '''\n",
    "    create a mesh in the zones total bbox at every step (in the units of the zones crs)\n",
    "    step: degree if crs=4326, else meters. 0.01 deg ~ 1km\n",
    "    '''\n",
    "    x_max, y_max = zones.bounds.max()[['maxx','maxy']].values\n",
    "    x_min, y_min = zones.bounds.min()[['minx','miny']].values\n",
    "\n",
    "    points = []\n",
    "    x = x_min\n",
    "    while x<x_max:\n",
    "        y = y_min\n",
    "        while y<y_max:\n",
    "            points.append(Point(x,y))\n",
    "            y += step\n",
    "        x += step\n",
    "    points = gpd.GeoDataFrame(geometry=points,crs=zones.crs)\n",
    "    points.index.name='index'\n",
    "    return points\n",
    "\n",
    "# https://stackoverflow.com/questions/36399381/whats-the-fastest-way-of-checking-if-a-point-is-inside-a-polygon-in-python\n",
    "\n",
    "@jit(nopython=True)\n",
    "def fast_point_in_polygon(x: float, y: float , poly: np.ndarray) -> bool:\n",
    "    n = len(poly)\n",
    "    inside = False\n",
    "    p2x = 0.0\n",
    "    p2y = 0.0\n",
    "    xints = 0.0\n",
    "    p1x,p1y = poly[0]\n",
    "    for i in nb.prange(n+1):\n",
    "        p2x,p2y = poly[i % n]\n",
    "        if y > min(p1y,p2y):\n",
    "            if y <= max(p1y,p2y):\n",
    "                if x <= max(p1x,p2x):\n",
    "                    if p1y != p2y:\n",
    "                        xints = (y-p1y)*(p2x-p1x)/(p2y-p1y)+p1x\n",
    "                    if p1x == p2x or x <= xints:\n",
    "                        inside = not inside\n",
    "        p1x,p1y = p2x,p2y\n",
    "        \n",
    "    return inside\n",
    "\n",
    "\n",
    "@njit(parallel=True)\n",
    "def fast_points_in_polygon(points:np.ndarray, polygon:np.ndarray) -> np.ndarray:\n",
    "    D = np.empty(len(points), dtype=nb.boolean) \n",
    "    for i in nb.prange(0, len(points)):\n",
    "        D[i] = fast_point_in_polygon(points[i,0], points[i,1], polygon)\n",
    "    return np.where(D)[0]\n",
    "\n",
    "def points_in_polygon(points:np.ndarray, polygon:gpd.GeoDataFrame) -> np.ndarray:\n",
    "    '''\n",
    "    return a list of point in the polygon. values are the index in the points array.\n",
    "    \n",
    "    points:np.array[np.array[float,float]]\n",
    "        list of all the points coords (x,y)\n",
    "    polygon: gpd.GeoDataFrame\n",
    "        geodataframe of multiples polygons.\n",
    "    '''\n",
    "    try:\n",
    "        poly = np.array([*polygon.exterior.coords])\n",
    "        return fast_points_in_polygon(points,poly)\n",
    "    except:\n",
    "        res=np.array([])\n",
    "        #polygon = polygon.geoms\n",
    "        for i in range(len(polygon)):\n",
    "            poly = np.array([*polygon[i].exterior.coords])\n",
    "            val =fast_points_in_polygon(points,poly)\n",
    "            res = np.append(res,val)\n",
    "        return res\n",
    "\n",
    "def population_to_mesh(population: gpd.GeoDataFrame,\n",
    "                       mesh: gpd.GeoDataFrame = None,\n",
    "                       step: float = 0.01,\n",
    "                       col: str = 'population', \n",
    "                       fill_missing: Literal['centroid', 'nearest', None] = 'centroid') ->  gpd.GeoDataFrame:\n",
    "    '''\n",
    "    create a mesh in the zones total bbox at every step (in the units of the zones crs)\n",
    "    and assign the population to each node equaly (if 2 node in a zone. they have each 50% of the population)\n",
    "    population:\n",
    "        geodataframe with total population by zones ans zones geomerty\n",
    "     mesh:\n",
    "        road nodes for example. if None. it will be created with equal step (variable step.)\n",
    "    step: \n",
    "        if mesh is None, Distance between each point degree if crs=4326, else meters. 0.01 deg ~ 1km\n",
    "    col:\n",
    "        column name with data to aggregation (population)\n",
    "    fill_missing: 'centroid', 'nearest', or None\n",
    "        centroid: zones centroid with no mesh node inside will be added to the mesh\n",
    "        nearest: zones population with no mesh point inside will be added to the nearest mesh point.\n",
    "    '''\n",
    "    import warnings\n",
    "    warnings.filterwarnings('ignore')\n",
    "    population=population.copy()\n",
    "    if population.index.name != 'index':\n",
    "        population.index.name = 'index'\n",
    "    # use existing mesh (points .geosjon) or create one.\n",
    "    if mesh is not None:\n",
    "        # we need numerical indexes. also,\n",
    "        # new nodes will be added (new index) for zones with no points inside.\n",
    "        points = mesh.copy()\n",
    "        points = points.reset_index(names='node_index')\n",
    "        points.index.name='index'\n",
    "    else:\n",
    "        points = create_mesh(population, step=step)\n",
    "        \n",
    "    points_coords = np.array([point.coords[0] for point in points['geometry'].values])\n",
    "    \n",
    "    population['nodes'] = population['geometry'].apply(lambda x: points_in_polygon(points_coords,x))\n",
    "    \n",
    "    nodes = population.reset_index()[['index','nodes',col]].copy()\n",
    "    nodes = nodes.explode('nodes').dropna()\n",
    "    print(len(nodes[nodes['nodes'].duplicated()]),'nodes in multiple zones. will be match to a single zone.')\n",
    "    \n",
    "    \n",
    "    zone_index_dict = nodes.set_index('nodes')['index'].to_dict()\n",
    "    points['zone'] = points.index.map(zone_index_dict)\n",
    "\n",
    "    pop_dict = nodes.set_index('nodes')[col].to_dict()\n",
    "    points[col] = points.index.map(pop_dict)\n",
    "    points = points.dropna()\n",
    "    \n",
    "    # get number of points per zones. divide population equaly between each points\n",
    "    len_dict = points.groupby('zone')[col].agg(len).to_dict()\n",
    "    points['num_points'] = points['zone'].apply(lambda x:len_dict.get(x))\n",
    "    points[col] = points[col] / points['num_points']\n",
    "    points = points.drop(columns = ['num_points'])\n",
    "    \n",
    "    print(len(population) - len(points['zone'].unique()),'unfounded zones')\n",
    "    \n",
    "    zones_list = points['zone'].unique()\n",
    "    unfounded_zones = population.loc[~population.index.isin(zones_list)][['geometry',col]]\n",
    "    if fill_missing == 'centroid':\n",
    "        print('Unfound zones centroid will be added to mesh')\n",
    "        # append unfounded zones centroids as in mesh\n",
    "        unfounded_zones['geometry'] = unfounded_zones.centroid\n",
    "        unfounded_zones = unfounded_zones.reset_index().rename(columns={'index':'zone'})\n",
    "        points = pd.concat([points,unfounded_zones]).reset_index(drop=True)\n",
    "        points.index.name='index'\n",
    "    elif fill_missing == 'nearest':\n",
    "        print('unfound zone will be added to nearest mesh node. zone_index will be lost')\n",
    "        unfounded_zones = zones_nearest_node(unfounded_zones,points)\n",
    "        pop_to_append = unfounded_zones.groupby('node_index')[[col]].sum()\n",
    "\n",
    "        points = points.merge(pop_to_append,left_index=True,right_index=True,how='left')\n",
    "        points[col+'_y'] = points[col+'_y'].fillna(0)\n",
    "\n",
    "        points[col] = points[col+'_x'] + points[col+'_y']\n",
    "        points = points.drop(columns=[col+'_x', col+'_y'])\n",
    "    else:\n",
    "        pass\n",
    "    \n",
    "    \n",
    "    points.index.name='index'\n",
    "    \n",
    "    return points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "5475f34b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_acf_distances(nodes: gpd.GeoDataFrame, \n",
    "                      mesh: gpd.GeoDataFrame, \n",
    "                      crs:int,\n",
    "                      max_dist: float = 3000) -> gpd.GeoDataFrame:\n",
    "    '''\n",
    "    with nearest kneibor in a radius.\n",
    "    for pt node in nodes, get all mesh nodes in a distance < max_dist\n",
    "    \n",
    "    return gpd.Geodateframe with [node_index, mesh_index, distances, population]\n",
    "    '''\n",
    "\n",
    "    node_dist = nearest_radius(nodes, mesh, radius=max_dist, to_crs=crs)\n",
    "    node_dist = node_dist.rename(columns={'ix_one': 'node_index','index_nn':'mesh_index'}).drop(columns='rank')\n",
    "\n",
    "    nodes_index_dict = nodes.reset_index()['index'].to_dict()\n",
    "    node_dist['node_index'] = node_dist['node_index'].apply(lambda x: nodes_index_dict.get(x))\n",
    "\n",
    "    node_dist = node_dist.explode(['mesh_index','distances'])\n",
    "    population_dict = mesh['population'].to_dict()\n",
    "    node_dist['population'] = node_dist['mesh_index'].apply(lambda x: population_dict.get(x))\n",
    "    return node_dist\n",
    "\n",
    "def get_routing_distances(nodes: gpd.GeoDataFrame, \n",
    "                         rnodes: gpd.GeoDataFrame, \n",
    "                         rlinks: gpd.GeoDataFrame, \n",
    "                         mesh: gpd.GeoDataFrame, \n",
    "                         weight_col:str = 'length', \n",
    "                         dijkstra_limit: float = np.inf) -> gpd.GeoDataFrame:\n",
    "    '''\n",
    "    with dijktra on road network.\n",
    "    for pt node in nodes, get all mesh nodes in a distance < max_dist. can be change with weight_col\n",
    "    ex: weight_col = 'time', and dijkstra_limit = 120secs\n",
    "    \n",
    "    return gpd.Geodateframe with [node_index, mesh_index, distances, population]\n",
    "    '''\n",
    "\n",
    "    # transform PT nodes to nearest road nodes\n",
    "    node_to_rnode_df = zones_nearest_node(nodes,rnodes)[['node_index']]\n",
    "\n",
    "    node_rnodes_dict = node_to_rnode_df['node_index'].to_dict()\n",
    "    rnodes_node_dict = node_to_rnode_df.reset_index().groupby('node_index').agg(list)['index'].to_dict()\n",
    "\n",
    "    # there may be multiples nodes pointing to the same rnode. so rnodes_node_dict values are lists.\n",
    "    # need to added them back at the end when we go from rnode to nodes\n",
    "    origins = list(set(node_rnodes_dict.values()))\n",
    "    destinations = mesh['node_index'].values\n",
    "    mat = routing(origins, destinations, rlinks, weight_col=weight_col, dijkstra_limit=dijkstra_limit)\n",
    "\n",
    "    mat = mat.merge(mesh.reset_index()[['index','node_index','population']],left_on='destination',right_on='node_index',how='left')\n",
    "    mat = mat.drop(columns=['destination','node_index']).rename(columns={'index':'mesh_index'})\n",
    "\n",
    "    mat['origin'] = mat['origin'].apply(lambda x: rnodes_node_dict.get(x))\n",
    "    mat = mat.explode('origin')\n",
    "    mat = mat.rename(columns={'origin':'node_index', weight_col:'distances'})\n",
    "    return mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "4e405b87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../../scenarios/montreal_2/inputs/pt/\n"
     ]
    }
   ],
   "source": [
    "base_folder = argv['training_folder']\n",
    "pt_folder = base_folder + '/inputs/pt/'\n",
    "road_folder = base_folder + '/inputs/road/'\n",
    "input_folder = base_folder +'/inputs/'\n",
    "od_folder = base_folder + '/inputs/od/'\n",
    "output_folder = base_folder +'/outputs/'\n",
    "print(pt_folder)\n",
    "if not os.path.exists(output_folder):\n",
    "    os.makedirs(output_folder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "0f3038cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "catchment_radius = argv['params' ]['catchment_radius']\n",
    "catchment_radius = {k:float(v) for k,v in catchment_radius.items()}\n",
    "default_catchment_radius = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81b11c40",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cdd032bd",
   "metadata": {},
   "source": [
    "# inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "7dadadbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#cst_incline = argv['params' ]['constant']['cst_incline']\n",
    "#cst_road = argv['params']['road_weight']\n",
    "#cst_shared = argv['params']['shared_cycleway_weight']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "1a674100",
   "metadata": {},
   "outputs": [],
   "source": [
    "links = gpd.read_file(pt_folder + 'links.geojson') \n",
    "nodes = gpd.read_file(pt_folder + 'nodes.geojson')\n",
    "links = links.set_index('index')\n",
    "nodes = nodes.set_index('index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "8c5c4543",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "population? True\n"
     ]
    }
   ],
   "source": [
    "population_file = input_folder + 'population.geojson'\n",
    "population_file_provided = os.path.isfile(population_file)\n",
    "if population_file_provided :\n",
    "    population = gpd.read_file(population_file)\n",
    "    if 'index' in population.columns:\n",
    "        population = population.set_index('index')\n",
    "    else:\n",
    "        population.index.name='index'\n",
    "    assert 'density' in population.columns, 'need density column. in km2'\n",
    "    assert population.crs == 4326, 'population.geojson CRS must be EPSG:4326'\n",
    "print('population?',population_file_provided)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "ac965c16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rnodes? False\n"
     ]
    }
   ],
   "source": [
    "rnodes_file = road_folder + 'road_nodes.geojson'\n",
    "rnodes_file_provided = os.path.isfile(rnodes_file)\n",
    "if rnodes_file_provided:\n",
    "    rnodes = gpd.read_file(road_folder + 'road_nodes.geojson')\n",
    "    rnodes = rnodes.set_index('index')\n",
    "    rlinks = gpd.read_file(road_folder + 'road_links.geojson')\n",
    "    rlinks = rlinks.set_index('index')\n",
    "print('rnodes?',rnodes_file_provided)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "6071cbdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "od? True\n"
     ]
    }
   ],
   "source": [
    "od_file = od_folder + 'od.geojson'\n",
    "od_file_provided = os.path.isfile(od_file)\n",
    "if od_file_provided:\n",
    "    od_test = gpd.read_file(od_folder + 'od.geojson')\n",
    "    if 'name' not in od_test.columns:\n",
    "        od_test['name'] = od_test['index']\n",
    "    od_test['name'] = od_test['name'].fillna(od_test['index'].astype(str))\n",
    "print('od?',od_file_provided)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08f6a33b",
   "metadata": {},
   "source": [
    "# population preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20859122",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "df034d31",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_662986/550055817.py:2: UserWarning: Geometry is in a geographic CRS. Results from 'centroid' are likely incorrect. Use 'GeoSeries.to_crs()' to re-project geometries to a projected CRS before this operation.\n",
      "\n",
      "  centroid = [*LineString(nodes.centroid.values).centroid.coords][0]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "32618"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# find meters CRS\n",
    "centroid = [*LineString(nodes.centroid.values).centroid.coords][0]\n",
    "crs = get_epsg(centroid[1],centroid[0])\n",
    "crs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "8178d208",
   "metadata": {},
   "outputs": [],
   "source": [
    "if population_file_provided : \n",
    "    population['area (km2)'] = population.to_crs(crs).area*1e-6\n",
    "    population['area (km2)'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "283a2929",
   "metadata": {},
   "outputs": [],
   "source": [
    "if population_file_provided :\n",
    "    population['population'] = population['density']*population['area (km2)']\n",
    "    population['population'].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "220bbb67",
   "metadata": {},
   "source": [
    "# population mesh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e6f6fe3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7458 nodes in multiple zones. will be match to a single zone.\n",
      "146 unfounded zones\n",
      "Unfound zones centroid will be added to mesh\n"
     ]
    }
   ],
   "source": [
    "if not population_file_provided:\n",
    "    mesh = gpd.GeoDataFrame(index=[0],data={'zone':'centroid','population':0},geometry=[Point(centroid[0],centroid[1])])\n",
    "    mesh.index.name = 'index'\n",
    "    mesh.crs=4326\n",
    "elif rnodes_file_provided:\n",
    "    # use rnodes as mesh.\n",
    "    print('using road_nodes')\n",
    "    mesh = population_to_mesh(population, mesh=rnodes, step=0.005, col='population', fill_missing='nearest')\n",
    "else:\n",
    "    # create a mesh\n",
    "    #0.01 = 1km 0.005 = 500m\n",
    "    mesh = population_to_mesh(population, step=0.005, col = 'population', fill_missing='centroid')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "6a276040",
   "metadata": {},
   "outputs": [],
   "source": [
    "mesh.to_file(output_folder + 'population_mesh.geojson',driver='GeoJSON')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfbafa49",
   "metadata": {},
   "source": [
    "# catchment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "aae1e149",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find TC nodes to mesh distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "ac854281",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_dist = max(max(catchment_radius.values()),default_catchment_radius)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "ffc2ca97",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if rnodes_file_provided:\n",
    "    print('using road_nodes')\n",
    "    node_dist = get_routing_distances(nodes, rnodes, rlinks, mesh, 'length', max_dist)\n",
    "else:\n",
    "    node_dist = get_acf_distances(nodes, mesh, crs, max_dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "496d9cfe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>node_index</th>\n",
       "      <th>mesh_index</th>\n",
       "      <th>distances</th>\n",
       "      <th>population</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>node_0</td>\n",
       "      <td>30824</td>\n",
       "      <td>965.622944</td>\n",
       "      <td>1222.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>node_0</td>\n",
       "      <td>31197</td>\n",
       "      <td>932.546563</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>node_0</td>\n",
       "      <td>31010</td>\n",
       "      <td>865.069029</td>\n",
       "      <td>1039.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>node_0</td>\n",
       "      <td>46008</td>\n",
       "      <td>601.03284</td>\n",
       "      <td>1469.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>node_0</td>\n",
       "      <td>30639</td>\n",
       "      <td>857.936823</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14350</th>\n",
       "      <td>node_14350</td>\n",
       "      <td>28436</td>\n",
       "      <td>754.161975</td>\n",
       "      <td>2216.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14350</th>\n",
       "      <td>node_14350</td>\n",
       "      <td>46074</td>\n",
       "      <td>686.919477</td>\n",
       "      <td>2005.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14350</th>\n",
       "      <td>node_14350</td>\n",
       "      <td>28795</td>\n",
       "      <td>862.593548</td>\n",
       "      <td>524.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14350</th>\n",
       "      <td>node_14350</td>\n",
       "      <td>28615</td>\n",
       "      <td>490.203278</td>\n",
       "      <td>3164.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14350</th>\n",
       "      <td>node_14350</td>\n",
       "      <td>28616</td>\n",
       "      <td>875.834551</td>\n",
       "      <td>1784.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>232843 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       node_index mesh_index   distances   population\n",
       "0          node_0      30824  965.622944  1222.000000\n",
       "0          node_0      31197  932.546563     0.000000\n",
       "0          node_0      31010  865.069029  1039.250000\n",
       "0          node_0      46008   601.03284  1469.000000\n",
       "0          node_0      30639  857.936823     0.000000\n",
       "...           ...        ...         ...          ...\n",
       "14350  node_14350      28436  754.161975  2216.333333\n",
       "14350  node_14350      46074  686.919477  2005.000000\n",
       "14350  node_14350      28795  862.593548   524.333333\n",
       "14350  node_14350      28615  490.203278  3164.000000\n",
       "14350  node_14350      28616  875.834551  1784.000000\n",
       "\n",
       "[232843 rows x 4 columns]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "node_dist"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3480e8ac",
   "metadata": {},
   "source": [
    "# metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "6ed69263",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num route_id: 198\n",
      "num route_type: 3\n"
     ]
    }
   ],
   "source": [
    "print('num route_id:',len(links['route_id'].unique()))\n",
    "print('num route_type:',len(links['route_type'].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "dc317930",
   "metadata": {},
   "outputs": [],
   "source": [
    "#init results dfs\n",
    "df_route_id = pd.DataFrame(index=links['route_id'].unique())\n",
    "df_route_id.index.name='route_id'\n",
    "\n",
    "df_route_type = pd.DataFrame(index=links['route_type'].unique())\n",
    "df_route_type.index.name='route_type'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "45bc4d79",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_catchment(col='route_id'):\n",
    "    #get all nodes with col filter\n",
    "    link = links.groupby(col)[['a','b','route_type']].agg({'a':set,'b':set,'route_type':'first'})\n",
    "    link['node'] = link.apply(lambda row: row['a'].union(row['b']), axis=1)\n",
    "    link = link.drop(columns=['a','b'])\n",
    "    # add catchment radius for the route_type\n",
    "    link['catchment_radius'] = link['route_type'].apply(lambda x: catchment_radius.get(x,default_catchment_radius))\n",
    "\n",
    "    col_exist = col == 'route_type' # cannot explode if index == route_type (a column)\n",
    "    link = link.explode('node').reset_index(drop=col_exist)\n",
    "    link = node_dist.merge(link, left_on='node_index', right_on='node')\n",
    "    #filter by distance\n",
    "    link = link[link['distances'] <= link['catchment_radius']]\n",
    "    #drop duplicated mesh nodes (we count only one time)\n",
    "    link = link.drop_duplicates(subset=['mesh_index',col],keep='first')\n",
    "\n",
    "    return link.groupby(col)['population'].sum().to_dict()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "e18e10d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11261888.89967505\n"
     ]
    }
   ],
   "source": [
    "res = get_catchment('route_id')\n",
    "\n",
    "df_route_id['catchment'] = res\n",
    "print(sum([item for key,item in res.items()]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "06d98402",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3201537.026203571\n"
     ]
    }
   ],
   "source": [
    "res = get_catchment('route_type')\n",
    "\n",
    "df_route_type['catchment'] = res\n",
    "print(sum([item for key,item in res.items()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d35b71b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "5ff69b2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    links['network']=True\n",
    "    res=[]\n",
    "    dists = [0,1,10,20,50,100,250,500,800,1000]\n",
    "    for dist in dists:\n",
    "        col='network'\n",
    "        link = links.groupby(col)[['a','b','route_type']].agg({'a':set,'b':set,'route_type':'first'})\n",
    "        link['node'] = link.apply(lambda row: row['a'].union(row['b']), axis=1)\n",
    "        link = link.drop(columns=['a','b'])\n",
    "        # add catchment radius for the route_type\n",
    "        link['catchment_radius'] =dist\n",
    "\n",
    "        col_exist = col == 'route_type' # cannot explode if index == route_type (a column)\n",
    "        link = link.explode('node').reset_index(drop=col_exist)\n",
    "        link = node_dist.merge(link, left_on='node_index', right_on='node')\n",
    "        #filter by distance\n",
    "        link = link[link['distances'] <= link['catchment_radius']]\n",
    "        #drop duplicated mesh nodes (we count only one time)\n",
    "        link = link.drop_duplicates(subset=['mesh_index',col],keep='first')\n",
    "        volume = link['population'].sum()\n",
    "        res.append(volume)\n",
    "    plt.plot(dists,res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1c2a4e2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "de5d12fd",
   "metadata": {},
   "source": [
    "# frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "3f251696",
   "metadata": {},
   "outputs": [],
   "source": [
    "links['frequency'] = 1/links['headway']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "3829349f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "627.2940713338257\n"
     ]
    }
   ],
   "source": [
    "res = (links.groupby('route_id')['frequency'].agg(np.nanmean)*3600).to_dict()\n",
    "\n",
    "df_route_id['frequency (veh/hours)'] = res\n",
    "print(np.nansum([item for key,item in res.items()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "e3035415",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14.946260789140776\n"
     ]
    }
   ],
   "source": [
    "res = (links.groupby('route_type')['frequency'].agg(np.nanmean)*3600).to_dict()\n",
    "\n",
    "df_route_type['frequency (veh/hours)'] = res\n",
    "print(sum([item for key,item in res.items()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "b1017844",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "499.8634917176481\n"
     ]
    }
   ],
   "source": [
    "link = (links.groupby(['route_id','trip_id'])[['frequency']].agg(np.nanmean)*3600)\n",
    "res = link.reset_index().set_index('route_id')['frequency'].to_dict()\n",
    "print(np.nansum([item for key,item in res.items()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "99bf3c5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15.779204535979321\n"
     ]
    }
   ],
   "source": [
    "link = (links.groupby(['route_type','trip_id'])[['frequency']].agg(np.nanmean)*3600)\n",
    "res = link.reset_index().set_index('route_type')['frequency'].to_dict()\n",
    "print(np.nansum([item for key,item in res.items()]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06a9b56e",
   "metadata": {},
   "source": [
    "# operational Fleet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "d73f2028",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_fleet(col='route_id'):\n",
    "    link = links.groupby([col,'trip_id'])[['time','frequency']].agg({'time':np.nansum,'frequency':np.nanmean})\n",
    "    link['fleet'] = np.ceil(link['frequency'] * link['time'])\n",
    "    return link.reset_index().groupby(col)['fleet'].agg(np.nansum).to_dict()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "88e247d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1059.0\n"
     ]
    }
   ],
   "source": [
    "res = get_fleet('route_id')\n",
    "\n",
    "df_route_id['fleet'] = res\n",
    "print(sum([item for key,item in res.items()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "0dc17813",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1059.0\n"
     ]
    }
   ],
   "source": [
    "res = get_fleet('route_type')\n",
    "\n",
    "df_route_type['fleet'] = res\n",
    "print(sum([item for key,item in res.items()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df7743de",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b0a16eb3",
   "metadata": {},
   "source": [
    "# Line Length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "5e08e66e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_length(col='route_id'):\n",
    "    link = links.groupby([col,'trip_id'])[[length_col]].agg(np.nansum)\n",
    "    return link.reset_index().groupby(col)[length_col].agg(np.nansum).to_dict()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "bfa2e75e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rpeparation. if legnth is NaN, or if shsape dist travel exist.\n",
    "\n",
    "length_col = None\n",
    "if 'length' in links.columns and length_col == None:\n",
    "    if len(links[links['length'].isnull()])==0:\n",
    "        length_col = 'length'\n",
    "        \n",
    "if 'shape_dist_traveled' in links.columns and length_col == None:\n",
    "    if len(links[links['shape_dist_traveled'].isnull()])==0:\n",
    "        length_col = 'shape_dist_traveled'\n",
    "\n",
    "if length_col == None:\n",
    "    print('create length from geometry')\n",
    "    links['length'] = links.to_crs(crs).length\n",
    "    length_col = 'length'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa1f7686",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "31db0758",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5297651.221627732\n"
     ]
    }
   ],
   "source": [
    "res = get_length('route_id')\n",
    "\n",
    "df_route_id['length (m)'] = res\n",
    "print(sum([item for key,item in res.items()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "39826011",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5297651.22162773\n"
     ]
    }
   ],
   "source": [
    "res = get_length('route_type')\n",
    "\n",
    "df_route_type['length (m)'] = res\n",
    "print(sum([item for key,item in res.items()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7f4f68b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "638013ee",
   "metadata": {},
   "source": [
    "# Number of station per line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "6bf1bd3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# o-->o-->o-->o and  o<--o<--o<--o\n",
    "# est-ce que j'ai 8 ou 4 stations ?\n",
    "# j'ai 4 stations par trip et 4 stations par route (si c'est les memes).\n",
    "# comment savoir si cest les memes. clustering?\n",
    "# pour linstant. on prend tous les noeds unique par route_id ou route_type (col='route_id', route_id)\n",
    "def get_num_station(col='route_id'):\n",
    "    link = links.groupby(col)[['a','b']].agg({'a':set,'b':set})\n",
    "    link['node_len'] = link.apply(lambda row: len(row['a'].union(row['b'])), axis=1)\n",
    "    return link['node_len'].to_dict()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "6f27ad9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14283\n"
     ]
    }
   ],
   "source": [
    "res = get_num_station('route_id')\n",
    "\n",
    "df_route_id['num station'] = res\n",
    "print(sum([item for key,item in res.items()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "ea89801d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14283\n"
     ]
    }
   ],
   "source": [
    "res = get_num_station('route_type')\n",
    "\n",
    "df_route_type['num station'] = res\n",
    "print(sum([item for key,item in res.items()]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f98a64c",
   "metadata": {},
   "source": [
    "# Vehicle revenue KM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "c70257d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_veh_kmh(col='route_id'):\n",
    "    link = links.groupby([col,'trip_id'])[[length_col,'frequency']].agg({length_col:np.nansum,'frequency':np.nanmean})\n",
    "    link['veh_km/h'] = np.ceil(link['frequency'] * link[length_col]) * 3600/1000 #to km/H\n",
    "    return link.reset_index().groupby(col)['veh_km/h'].agg(np.nansum).to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "6b53c067",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16477.199999999993\n"
     ]
    }
   ],
   "source": [
    "res = get_veh_kmh('route_id')\n",
    "\n",
    "df_route_id['veh.km/h'] = res\n",
    "print(sum([item for key,item in res.items()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "3bc451d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16477.2\n"
     ]
    }
   ],
   "source": [
    "res = get_veh_kmh('route_type')\n",
    "\n",
    "df_route_type['veh.km/h'] = res\n",
    "print(sum([item for key,item in res.items()]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab77743f",
   "metadata": {},
   "source": [
    "# Round trip time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "fd818c88",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_round_trip_time(col='route_id'):\n",
    "    link = links.groupby([col,'trip_id'])[['time']].agg(np.nansum)\n",
    "    return link.reset_index().groupby(col)['time'].agg(np.nansum).to_dict()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "869ea67e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "957120\n"
     ]
    }
   ],
   "source": [
    "res = get_round_trip_time('route_id')\n",
    "\n",
    "df_route_id['round trip time (s)'] = res\n",
    "print(sum([item for key,item in res.items()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ac6bf88",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6a5213bf",
   "metadata": {},
   "source": [
    "# export dfs to csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "4a7d0d1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# round numbers\n",
    "for col in ['catchment', 'frequency (veh/hours)','length (m)','veh.km/h','round trip time (s)']:\n",
    "    df_route_id[col] = df_route_id[col].apply(lambda x :np.round(x,2))\n",
    "    df_route_id[col] = df_route_id[col].apply(lambda x :np.round(x,2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "3105f92e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_route_id = df_route_id.fillna('null')\n",
    "#df_route_type = df_route_type.fillna('null')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "984694c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>catchment</th>\n",
       "      <th>frequency (veh/hours)</th>\n",
       "      <th>fleet</th>\n",
       "      <th>length (m)</th>\n",
       "      <th>num station</th>\n",
       "      <th>veh.km/h</th>\n",
       "      <th>round trip time (s)</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>route_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>STM_100</th>\n",
       "      <td>37422.33</td>\n",
       "      <td>4.81</td>\n",
       "      <td>8.0</td>\n",
       "      <td>29574.32</td>\n",
       "      <td>76</td>\n",
       "      <td>162.0</td>\n",
       "      <td>4500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>STM_101</th>\n",
       "      <td>28147.83</td>\n",
       "      <td>2.01</td>\n",
       "      <td>2.0</td>\n",
       "      <td>6171.88</td>\n",
       "      <td>33</td>\n",
       "      <td>14.4</td>\n",
       "      <td>1260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>STM_102</th>\n",
       "      <td>57685.80</td>\n",
       "      <td>1.85</td>\n",
       "      <td>2.0</td>\n",
       "      <td>11802.12</td>\n",
       "      <td>51</td>\n",
       "      <td>25.2</td>\n",
       "      <td>2640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>STM_103</th>\n",
       "      <td>52248.40</td>\n",
       "      <td>5.30</td>\n",
       "      <td>5.0</td>\n",
       "      <td>10633.99</td>\n",
       "      <td>49</td>\n",
       "      <td>64.8</td>\n",
       "      <td>2400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>STM_104</th>\n",
       "      <td>80593.06</td>\n",
       "      <td>2.99</td>\n",
       "      <td>6.0</td>\n",
       "      <td>21512.17</td>\n",
       "      <td>86</td>\n",
       "      <td>68.4</td>\n",
       "      <td>5340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TRAINS_6</th>\n",
       "      <td>235700.32</td>\n",
       "      <td>0.66</td>\n",
       "      <td>4.0</td>\n",
       "      <td>137928.23</td>\n",
       "      <td>57</td>\n",
       "      <td>93.6</td>\n",
       "      <td>13320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>STM_1</th>\n",
       "      <td>260456.43</td>\n",
       "      <td>15.94</td>\n",
       "      <td>21.0</td>\n",
       "      <td>40144.82</td>\n",
       "      <td>54</td>\n",
       "      <td>644.4</td>\n",
       "      <td>4380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>STM_2</th>\n",
       "      <td>351634.40</td>\n",
       "      <td>8.29</td>\n",
       "      <td>27.0</td>\n",
       "      <td>103347.89</td>\n",
       "      <td>118</td>\n",
       "      <td>896.4</td>\n",
       "      <td>10980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>STM_4</th>\n",
       "      <td>20802.00</td>\n",
       "      <td>14.23</td>\n",
       "      <td>4.0</td>\n",
       "      <td>7661.44</td>\n",
       "      <td>6</td>\n",
       "      <td>111.6</td>\n",
       "      <td>720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>STM_5</th>\n",
       "      <td>164922.10</td>\n",
       "      <td>13.26</td>\n",
       "      <td>8.0</td>\n",
       "      <td>18992.03</td>\n",
       "      <td>24</td>\n",
       "      <td>255.6</td>\n",
       "      <td>1860</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>198 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          catchment  frequency (veh/hours)  fleet  length (m)  num station  \\\n",
       "route_id                                                                     \n",
       "STM_100    37422.33                   4.81    8.0    29574.32           76   \n",
       "STM_101    28147.83                   2.01    2.0     6171.88           33   \n",
       "STM_102    57685.80                   1.85    2.0    11802.12           51   \n",
       "STM_103    52248.40                   5.30    5.0    10633.99           49   \n",
       "STM_104    80593.06                   2.99    6.0    21512.17           86   \n",
       "...             ...                    ...    ...         ...          ...   \n",
       "TRAINS_6  235700.32                   0.66    4.0   137928.23           57   \n",
       "STM_1     260456.43                  15.94   21.0    40144.82           54   \n",
       "STM_2     351634.40                   8.29   27.0   103347.89          118   \n",
       "STM_4      20802.00                  14.23    4.0     7661.44            6   \n",
       "STM_5     164922.10                  13.26    8.0    18992.03           24   \n",
       "\n",
       "          veh.km/h  round trip time (s)  \n",
       "route_id                                 \n",
       "STM_100      162.0                 4500  \n",
       "STM_101       14.4                 1260  \n",
       "STM_102       25.2                 2640  \n",
       "STM_103       64.8                 2400  \n",
       "STM_104       68.4                 5340  \n",
       "...            ...                  ...  \n",
       "TRAINS_6      93.6                13320  \n",
       "STM_1        644.4                 4380  \n",
       "STM_2        896.4                10980  \n",
       "STM_4        111.6                  720  \n",
       "STM_5        255.6                 1860  \n",
       "\n",
       "[198 rows x 7 columns]"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_route_id.to_csv(output_folder+'route_id_metrics.csv')\n",
    "df_route_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "c6a5c283",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>catchment</th>\n",
       "      <th>frequency (veh/hours)</th>\n",
       "      <th>fleet</th>\n",
       "      <th>length (m)</th>\n",
       "      <th>num station</th>\n",
       "      <th>veh.km/h</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>route_type</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>bus</th>\n",
       "      <td>1.969423e+06</td>\n",
       "      <td>3.080261</td>\n",
       "      <td>980.0</td>\n",
       "      <td>4.593510e+06</td>\n",
       "      <td>13904</td>\n",
       "      <td>14115.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rail</th>\n",
       "      <td>5.739898e+05</td>\n",
       "      <td>0.807331</td>\n",
       "      <td>19.0</td>\n",
       "      <td>5.339954e+05</td>\n",
       "      <td>177</td>\n",
       "      <td>453.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>subway</th>\n",
       "      <td>6.581238e+05</td>\n",
       "      <td>11.058669</td>\n",
       "      <td>60.0</td>\n",
       "      <td>1.701462e+05</td>\n",
       "      <td>202</td>\n",
       "      <td>1908.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               catchment  frequency (veh/hours)  fleet    length (m)  \\\n",
       "route_type                                                             \n",
       "bus         1.969423e+06               3.080261  980.0  4.593510e+06   \n",
       "rail        5.739898e+05               0.807331   19.0  5.339954e+05   \n",
       "subway      6.581238e+05              11.058669   60.0  1.701462e+05   \n",
       "\n",
       "            num station  veh.km/h  \n",
       "route_type                         \n",
       "bus               13904   14115.6  \n",
       "rail                177     453.6  \n",
       "subway              202    1908.0  "
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_route_type.to_csv(output_folder+'route_type_metrics.csv')\n",
    "df_route_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42ad4b64",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9dc7238",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba0c4f26",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4924d2a1",
   "metadata": {},
   "source": [
    "# geomatic outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "10166c45",
   "metadata": {},
   "outputs": [],
   "source": [
    "#using get catchment. get the catchment radius of each node (get larger one if used by many mode.)\n",
    "link = links.groupby('route_type')[['a','b','route_type']].agg({'a':set,'b':set,'route_type':'first'})\n",
    "link['node'] = link.apply(lambda row: row['a'].union(row['b']), axis=1)\n",
    "link = link.drop(columns=['a','b'])\n",
    "# add catchment radius for the route_type\n",
    "link['catchment_radius'] = link['route_type'].apply(lambda x: catchment_radius.get(x,default_catchment_radius))\n",
    "link = link.explode('node').reset_index(drop=True)\n",
    "link = link.sort_values('catchment_radius',ascending=False).drop_duplicates('node',keep='first')\n",
    "link = node_dist.merge(link, left_on='node_index', right_on='node')\n",
    "link = link[link['distances'] <= link['catchment_radius']]\n",
    "\n",
    "temp_dict = link.groupby('node_index')['population'].sum().to_dict()\n",
    "nodes['catchment'] = nodes.index.map(temp_dict.get)\n",
    "\n",
    "temp_dict = link.groupby('node_index')['catchment_radius'].agg('first').to_dict() \n",
    "nodes['catchment_radius'] = nodes.index.map(temp_dict.get)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "db8164da",
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes.to_file(output_folder+'nodes.geojson',driver='GeoJSON')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11dfe2fb",
   "metadata": {},
   "source": [
    "# test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36fa7ac6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "637baab8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "quetzal_env",
   "language": "python",
   "name": "quetzal_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
