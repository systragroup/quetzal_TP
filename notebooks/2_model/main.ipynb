{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "71ccff01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'training_folder': '../../scenarios/base', 'params': {'catchment_radius': {'bus': '500', 'subway': '800', 'rail': '1000'}}}\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import json\n",
    "\n",
    "catchment_radius={'bus':'500','subway':'800','rail':'1000'}\n",
    "\n",
    "params = {'catchment_radius':catchment_radius}\n",
    "         \n",
    "default = {'training_folder': '../../scenarios/base', 'params':params} # Default execution parameters\n",
    "manual, argv = (True, default) if 'ipykernel' in sys.argv[0] else (False, dict(default, **json.loads(sys.argv[1])))\n",
    "print(argv)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ee5e2d8",
   "metadata": {},
   "source": [
    "io (read): 10 secs <br>\n",
    "mesh: 5-9 secs <br>\n",
    "acf_dist : 6-9 secs <br>\n",
    "metrics: 1.2 secs<br>\n",
    "<br>\n",
    "tot: 22-30 seconds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "99b5db61",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "sys.path.insert(0, r'../../../quetzal') # Add path to quetzal\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from shapely.geometry import Point, LineString\n",
    "from syspy.spatial.spatial import add_geometry_coordinates, nearest\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from typing import Literal\n",
    "from numba import jit, njit\n",
    "import numba as nb\n",
    "#num_cores = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "dc057466",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from quetzal_cyclops\n",
    "def get_epsg(lat: float, lon: float) -> int:\n",
    "    '''\n",
    "    lat, lon or y, x\n",
    "    return EPSG in meter for a given (lat,lon)\n",
    "    lat is north south \n",
    "    lon is est west\n",
    "    '''\n",
    "    return int(32700 - round((45 + lat) / 90, 0) * 100 + round((183 + lon) / 6, 0))\n",
    "\n",
    "# from quetzal_cyclops\n",
    "def zones_nearest_node(zones,nodes,drop_duplicates=False):\n",
    "    # getting zones centroids\n",
    "    centroid = zones.copy()\n",
    "    centroid['geometry'] = centroid.centroid\n",
    "    # finding nearest node\n",
    "    neigh = nearest(centroid, nodes, n_neighbors=1).rename(columns={'ix_one': 'zone_index', 'ix_many': 'node_index'})\n",
    "    zone_node_dict = neigh.set_index('zone_index')['node_index'].to_dict()\n",
    "    centroid['node_index'] = centroid.index.map(zone_node_dict.get)\n",
    "    #print('max_distance found: ', neigh['distance'].max())\n",
    "    # check for duplicated nodes. if there is. drop the duplicated zones.\n",
    "    if drop_duplicates:\n",
    "        if len(centroid.drop_duplicates('node_index')) != len(centroid):\n",
    "            print('there is zones associates to the same road_node')\n",
    "            # duplicated = centroid[centroid['node_index'].duplicated()]['node_index'].values\n",
    "            print('dropping zones: ')\n",
    "            print(centroid[centroid['node_index'].duplicated()].index.values)\n",
    "            centroid = centroid.drop_duplicates('node_index')\n",
    "    return centroid\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "2595e6cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from quetzal.engine.pathfinder_utils import simple_routing,sparse_matrix\n",
    "from scipy.sparse import csr_matrix\n",
    "from scipy.sparse.csgraph import dijkstra\n",
    "\n",
    "@jit(nopython=True)\n",
    "def _unstack(mat):\n",
    "    # return non inf values in mat as [[row,col,val],[row,col,val]]. so, [o,d,val].\n",
    "    # pd.DataFrame of this gives us [origin, destination, value] as columns\n",
    "    row, col = np.where(np.isfinite(mat))\n",
    "    res = np.zeros((len(row),3))\n",
    "    for it in nb.prange(len(col)):\n",
    "        i=row[it]\n",
    "        j=col[it]\n",
    "        d=mat[i,j]\n",
    "        res[it]=[i,j,d]\n",
    "    return res\n",
    "\n",
    "def routing(origin, destination, links, weight_col='time', dijkstra_limit=np.inf):\n",
    "    mat, node_index = sparse_matrix(links[['a', 'b', weight_col]].values)\n",
    "    index_node = {v: k for k, v in node_index.items()}\n",
    "    # liste des origines pour le dijkstra\n",
    "    origin_sparse = [node_index[x] for x in origin]\n",
    "    origin_dict =  {i:val for i,val in enumerate(origin_sparse)}\n",
    "    # list des destinations \n",
    "    destination_sparse = [node_index[x] for x in destination]\n",
    "    destination_dict =  {i:val for i,val in enumerate(destination_sparse)}\n",
    "    # dijktra on the road network from node = incices to every other nodes.\n",
    "    # from b to a.\n",
    "    dist_matrix = dijkstra(\n",
    "        csgraph=mat,\n",
    "        directed=True,\n",
    "        indices=origin_sparse,\n",
    "        return_predecessors=False,\n",
    "        limit=dijkstra_limit\n",
    "    )\n",
    "    # remove non-used destination\n",
    "    dist_matrix = dist_matrix[:,destination_sparse]\n",
    "    # unstack amtrix\n",
    "    dist_matrix = pd.DataFrame(_unstack(dist_matrix),columns=['origin', 'destination', weight_col])\n",
    "    # rename origin and destination with original indexes.\n",
    "    dist_matrix['origin'] = dist_matrix['origin'].apply(lambda x: index_node.get(origin_dict.get(x)))\n",
    "    dist_matrix['destination'] = dist_matrix['destination'].apply(lambda x: index_node.get(destination_dict.get(x)))\n",
    "    return dist_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "bb087e97",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_catchment_dist(link: gpd.GeoDataFrame, catchment_radius: dict, default: float=500):\n",
    "    route_type = link['route_type'].unique()\n",
    "    if len(route_type)>1:\n",
    "        print('multiple route type for a single route_id.. using first one for catchment radius')\n",
    "    route_type = route_type[0]\n",
    "    return catchment_radius.get(route_type, default)\n",
    "\n",
    "\n",
    "def nearest_radius(one, many, radius=100, geometry=False):\n",
    "    try:\n",
    "        # Assert df_many.index.is_unique\n",
    "        assert one.index.is_unique\n",
    "        assert many.index.is_unique\n",
    "    except AssertionError:\n",
    "        msg = 'Index of one or many should not contain duplicates'\n",
    "        print(msg)\n",
    "        warnings.warn(msg)\n",
    "    many = add_geometry_coordinates(many, columns=['x_geometry', 'y_geometry'], add_centroids=False)\n",
    "    one = add_geometry_coordinates(one, columns=['x_geometry', 'y_geometry'], add_centroids=False)\n",
    "    \n",
    "    x = many[['x_geometry', 'y_geometry']].values\n",
    "    # Fit Nearest neighbors model\n",
    "    #nbrs = NearestNeighbors(n_neighbors=n_neighbors, algorithm='ball_tree').fit(x)\n",
    "    nbrs = NearestNeighbors(radius=radius,algorithm='ball_tree').fit(x)\n",
    "\n",
    "\n",
    "    # x = df_many[['x_geometry','y_geometry']].values\n",
    "    y = one[['x_geometry', 'y_geometry']].values\n",
    "\n",
    "    #distances, indices = nbrs.kneighbors(y,return_distance=True)\n",
    "    distances, indices = nbrs.radius_neighbors(y, radius = radius, return_distance=True)\n",
    "\n",
    "    indices = pd.DataFrame(indices)\n",
    "    indices = pd.DataFrame(indices.stack(), columns=['index_nn']).reset_index().rename(\n",
    "        columns={'level_0': 'ix_one', 'level_1': 'rank'}\n",
    "    )\n",
    "    indices['distances'] = distances\n",
    "    return indices\n",
    "\n",
    "def create_mesh(zones: gpd.GeoDataFrame ,step: float = 0.01) -> gpd.GeoDataFrame:\n",
    "    '''\n",
    "    create a mesh in the zones total bbox at every step (in the units of the zones crs)\n",
    "    step: degree if crs=4326, else meters. 0.01 deg ~ 1km\n",
    "    '''\n",
    "    x_max, y_max = zones.bounds.max()[['maxx','maxy']].values\n",
    "    x_min, y_min = zones.bounds.min()[['minx','miny']].values\n",
    "\n",
    "    points = []\n",
    "    x = x_min\n",
    "    while x<x_max:\n",
    "        y = y_min\n",
    "        while y<y_max:\n",
    "            points.append(Point(x,y))\n",
    "            y += step\n",
    "        x += step\n",
    "    points = gpd.GeoDataFrame(geometry=points,crs=zones.crs)\n",
    "    points.index.name='index'\n",
    "    return points\n",
    "\n",
    "# https://stackoverflow.com/questions/36399381/whats-the-fastest-way-of-checking-if-a-point-is-inside-a-polygon-in-python\n",
    "\n",
    "@jit(nopython=True)\n",
    "def fast_point_in_polygon(x: float, y: float , poly: np.ndarray) -> bool:\n",
    "    n = len(poly)\n",
    "    inside = False\n",
    "    p2x = 0.0\n",
    "    p2y = 0.0\n",
    "    xints = 0.0\n",
    "    p1x,p1y = poly[0]\n",
    "    for i in nb.prange(n+1):\n",
    "        p2x,p2y = poly[i % n]\n",
    "        if y > min(p1y,p2y):\n",
    "            if y <= max(p1y,p2y):\n",
    "                if x <= max(p1x,p2x):\n",
    "                    if p1y != p2y:\n",
    "                        xints = (y-p1y)*(p2x-p1x)/(p2y-p1y)+p1x\n",
    "                    if p1x == p2x or x <= xints:\n",
    "                        inside = not inside\n",
    "        p1x,p1y = p2x,p2y\n",
    "        \n",
    "    return inside\n",
    "\n",
    "\n",
    "@njit(parallel=True)\n",
    "def fast_points_in_polygon(points:np.ndarray, polygon:np.ndarray) -> np.ndarray:\n",
    "    D = np.empty(len(points), dtype=nb.boolean) \n",
    "    for i in nb.prange(0, len(points)):\n",
    "        D[i] = fast_point_in_polygon(points[i,0], points[i,1], polygon)\n",
    "    return np.where(D)[0]\n",
    "\n",
    "def points_in_polygon(points:np.ndarray, polygon:gpd.GeoDataFrame) -> np.ndarray:\n",
    "    '''\n",
    "    return a list of point in the polygon. values are the index in the points array.\n",
    "    \n",
    "    points:np.array[np.array[float,float]]\n",
    "        list of all the points coords (x,y)\n",
    "    polygon: gpd.GeoDataFrame\n",
    "        geodataframe of multiples polygons.\n",
    "    '''\n",
    "    polygon = np.array([*polygon.exterior.coords])\n",
    "    return fast_points_in_polygon(points,polygon)\n",
    "\n",
    "def population_to_mesh(population: gpd.GeoDataFrame,\n",
    "                       mesh: gpd.GeoDataFrame = None,\n",
    "                       step: float = 0.01,\n",
    "                       col: str = 'population', \n",
    "                       fill_missing: Literal['centroid', 'nearest', None] = 'centroid') ->  gpd.GeoDataFrame:\n",
    "    '''\n",
    "    create a mesh in the zones total bbox at every step (in the units of the zones crs)\n",
    "    and assign the population to each node equaly (if 2 node in a zone. they have each 50% of the population)\n",
    "    population:\n",
    "        geodataframe with total population by zones ans zones geomerty\n",
    "     mesh:\n",
    "        road nodes for example. if None. it will be created with equal step (variable step.)\n",
    "    step: \n",
    "        if mesh is None, Distance between each point degree if crs=4326, else meters. 0.01 deg ~ 1km\n",
    "    col:\n",
    "        column name with data to aggregation (population)\n",
    "    fill_missing: 'centroid', 'nearest', or None\n",
    "        centroid: zones centroid with no mesh node inside will be added to the mesh\n",
    "        nearest: zones population with no mesh point inside will be added to the nearest mesh point.\n",
    "    '''\n",
    "    import warnings\n",
    "    warnings.filterwarnings('ignore')\n",
    "    population=population.copy()\n",
    "    if population.index.name != 'index':\n",
    "        population.index.name = 'index'\n",
    "    # use existing mesh (points .geosjon) or create one.\n",
    "    if mesh is not None:\n",
    "        # we need numerical indexes. also,\n",
    "        # new nodes will be added (new index) for zones with no points inside.\n",
    "        points = mesh.copy()\n",
    "        points = points.reset_index(names='node_index')\n",
    "        points.index.name='index'\n",
    "    else:\n",
    "        points = create_mesh(population, step=step)\n",
    "        \n",
    "    points_coords = np.array([point.coords[0] for point in points['geometry'].values])\n",
    "    \n",
    "    population['nodes'] = population['geometry'].apply(lambda x: points_in_polygon(points_coords,x))\n",
    "    \n",
    "    nodes = population.reset_index()[['index','nodes',col]].copy()\n",
    "    nodes = nodes.explode('nodes').dropna()\n",
    "    print(len(nodes[nodes['nodes'].duplicated()]),'nodes in multiple zones. will be match to a single zone.')\n",
    "    \n",
    "    \n",
    "    zone_index_dict = nodes.set_index('nodes')['index'].to_dict()\n",
    "    points['zone'] = points.index.map(zone_index_dict)\n",
    "\n",
    "    pop_dict = nodes.set_index('nodes')[col].to_dict()\n",
    "    points[col] = points.index.map(pop_dict)\n",
    "    points = points.dropna()\n",
    "    \n",
    "    # get number of points per zones. divide population equaly between each points\n",
    "    len_dict = points.groupby('zone')[col].agg(len).to_dict()\n",
    "    points['num_points'] = points['zone'].apply(lambda x:len_dict.get(x))\n",
    "    points[col] = points[col] / points['num_points']\n",
    "    points = points.drop(columns = ['num_points'])\n",
    "    \n",
    "    print(len(population) - len(points['zone'].unique()),'unfounded zones')\n",
    "    \n",
    "    zones_list = points['zone'].unique()\n",
    "    unfounded_zones = population.loc[~population.index.isin(zones_list)][['geometry',col]]\n",
    "    if fill_missing == 'centroid':\n",
    "        print('Unfound zones centroid will be added to mesh')\n",
    "        # append unfounded zones centroids as in mesh\n",
    "        unfounded_zones['geometry'] = unfounded_zones.centroid\n",
    "        unfounded_zones = unfounded_zones.reset_index().rename(columns={'index':'zone'})\n",
    "        points = pd.concat([points,unfounded_zones]).reset_index(drop=True)\n",
    "        points.index.name='index'\n",
    "    elif fill_missing == 'nearest':\n",
    "        print('unfound zone will be added to nearest mesh node. zone_index will be lost')\n",
    "        unfounded_zones = zones_nearest_node(unfounded_zones,points)\n",
    "        pop_to_append = unfounded_zones.groupby('node_index')[[col]].sum()\n",
    "\n",
    "        points = points.merge(pop_to_append,left_index=True,right_index=True,how='left')\n",
    "        points[col+'_y'] = points[col+'_y'].fillna(0)\n",
    "\n",
    "        points[col] = points[col+'_x'] + points[col+'_y']\n",
    "        points = points.drop(columns=[col+'_x', col+'_y'])\n",
    "    else:\n",
    "        pass\n",
    "    \n",
    "    \n",
    "    points.index.name='index'\n",
    "    \n",
    "    return points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "5475f34b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_acf_distances(nodes: gpd.GeoDataFrame, \n",
    "                      mesh: gpd.GeoDataFrame, \n",
    "                      crs:int,\n",
    "                      max_dist: float = 3000) -> gpd.GeoDataFrame:\n",
    "    '''\n",
    "    with nearest kneibor in a radius.\n",
    "    for pt node in nodes, get all mesh nodes in a distance < max_dist\n",
    "    \n",
    "    return gpd.Geodateframe with [node_index, mesh_index, distances, population]\n",
    "    '''\n",
    "\n",
    "    node_dist = nearest_radius(nodes.to_crs(crs), mesh.to_crs(crs), radius=max_dist)\n",
    "    node_dist = node_dist.rename(columns={'ix_one': 'node_index','index_nn':'mesh_index'}).drop(columns='rank')\n",
    "\n",
    "    nodes_index_dict = nodes.reset_index()['index'].to_dict()\n",
    "    node_dist['node_index'] = node_dist['node_index'].apply(lambda x: nodes_index_dict.get(x))\n",
    "\n",
    "    node_dist = node_dist.explode(['mesh_index','distances'])\n",
    "    population_dict = mesh['population'].to_dict()\n",
    "    node_dist['population'] = node_dist['mesh_index'].apply(lambda x: population_dict.get(x))\n",
    "    return node_dist\n",
    "\n",
    "def get_routing_distances(nodes: gpd.GeoDataFrame, \n",
    "                         rnodes: gpd.GeoDataFrame, \n",
    "                         rlinks: gpd.GeoDataFrame, \n",
    "                         mesh: gpd.GeoDataFrame, \n",
    "                         weight_col:str = 'length', \n",
    "                         dijkstra_limit: float = np.inf) -> gpd.GeoDataFrame:\n",
    "    '''\n",
    "    with dijktra on road network.\n",
    "    for pt node in nodes, get all mesh nodes in a distance < max_dist. can be change with weight_col\n",
    "    ex: weight_col = 'time', and dijkstra_limit = 120secs\n",
    "    \n",
    "    return gpd.Geodateframe with [node_index, mesh_index, distances, population]\n",
    "    '''\n",
    "\n",
    "    # transform PT nodes to nearest road nodes\n",
    "    node_to_rnode_df = zones_nearest_node(nodes,rnodes)[['node_index']]\n",
    "\n",
    "    node_rnodes_dict = node_to_rnode_df['node_index'].to_dict()\n",
    "    rnodes_node_dict = node_to_rnode_df.reset_index().groupby('node_index').agg(list)['index'].to_dict()\n",
    "\n",
    "    # there may be multiples nodes pointing to the same rnode. so rnodes_node_dict values are lists.\n",
    "    # need to added them back at the end when we go from rnode to nodes\n",
    "    origins = list(set(node_rnodes_dict.values()))\n",
    "    destinations = mesh['node_index'].values\n",
    "    mat = routing(origins, destinations, rlinks, weight_col=weight_col, dijkstra_limit=dijkstra_limit)\n",
    "\n",
    "    mat = mat.merge(mesh.reset_index()[['index','node_index','population']],left_on='destination',right_on='node_index',how='left')\n",
    "    mat = mat.drop(columns=['destination','node_index']).rename(columns={'index':'mesh_index'})\n",
    "\n",
    "    mat['origin'] = mat['origin'].apply(lambda x: rnodes_node_dict.get(x))\n",
    "    mat = mat.explode('origin')\n",
    "    mat = mat.rename(columns={'origin':'node_index', weight_col:'distances'})\n",
    "    return mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "4e405b87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../../scenarios/base/inputs/pt/\n"
     ]
    }
   ],
   "source": [
    "base_folder = argv['training_folder']\n",
    "pt_folder = base_folder + '/inputs/pt/'\n",
    "road_folder = base_folder + '/inputs/road/'\n",
    "input_folder = base_folder +'/inputs/'\n",
    "od_folder = base_folder + '/inputs/od/'\n",
    "output_folder = base_folder +'/outputs/'\n",
    "print(pt_folder)\n",
    "if not os.path.exists(output_folder):\n",
    "    os.makedirs(output_folder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "0f3038cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "catchment_radius = argv['params' ]['catchment_radius']\n",
    "catchment_radius = {k:float(v) for k,v in catchment_radius.items()}\n",
    "default_catchment_radius = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81b11c40",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cdd032bd",
   "metadata": {},
   "source": [
    "# inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "7dadadbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#cst_incline = argv['params' ]['constant']['cst_incline']\n",
    "#cst_road = argv['params']['road_weight']\n",
    "#cst_shared = argv['params']['shared_cycleway_weight']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "1a674100",
   "metadata": {},
   "outputs": [],
   "source": [
    "links = gpd.read_file(pt_folder + 'links.geojson') \n",
    "nodes = gpd.read_file(pt_folder + 'nodes.geojson')\n",
    "links = links.set_index('index')\n",
    "nodes = nodes.set_index('index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "8c5c4543",
   "metadata": {},
   "outputs": [],
   "source": [
    "population = gpd.read_file(input_folder + 'population.geojson')\n",
    "if 'index' in population.columns:\n",
    "    population = population.set_index('index')\n",
    "else:\n",
    "    population.index.name='index'\n",
    "assert 'density' in population.columns, 'need density column. in km2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88a30058",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "ac965c16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rnodes? False\n"
     ]
    }
   ],
   "source": [
    "rnodes_file = road_folder + 'road_nodes.geojson'\n",
    "rnodes_file_provided = os.path.isfile(rnodes_file)\n",
    "if rnodes_file_provided:\n",
    "    rnodes = gpd.read_file(road_folder + 'road_nodes.geojson')\n",
    "    rnodes = rnodes.set_index('index')\n",
    "    rlinks = gpd.read_file(road_folder + 'road_links.geojson')\n",
    "    rlinks = rlinks.set_index('index')\n",
    "print('rnodes?',rnodes_file_provided)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "6071cbdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "od_file = od_folder + 'od.geojson'\n",
    "od_file_provided = os.path.isfile(od_file)\n",
    "if od_file_provided:\n",
    "    od_test = gpd.read_file(od_folder + 'od.geojson')\n",
    "    if 'name' not in od_test.columns:\n",
    "        od_test['name'] = od_test['index']\n",
    "    od_test['name'] = od_test['name'].fillna(od_test['index'].astype(str))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08f6a33b",
   "metadata": {},
   "source": [
    "# population preapation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "df034d31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32618"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# find meters CRS\n",
    "centroid = [*LineString(population.centroid.values).centroid.coords][0]\n",
    "crs = get_epsg(centroid[1],centroid[0])\n",
    "crs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "8178d208",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11574.507300457291"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "population['area (km2)'] = population.to_crs(crs).area*1e-6\n",
    "population['area (km2)'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "283a2929",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4884999.000000022"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "population['population'] = population['density']*population['area (km2)']\n",
    "population['population'].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "220bbb67",
   "metadata": {},
   "source": [
    "# population mesh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "e6f6fe3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7458 nodes in multiple zones. will be match to a single zone.\n",
      "146 unfounded zones\n",
      "Unfound zones centroid will be added to mesh\n"
     ]
    }
   ],
   "source": [
    "if rnodes_file_provided:\n",
    "    # use rnodes as mesh.\n",
    "    print('using road_nodes')\n",
    "    mesh = population_to_mesh(population, mesh=rnodes, step=0.005, col='population', fill_missing='nearest')\n",
    "else:\n",
    "    # create a mesh\n",
    "    #0.01 = 1km 0.005 = 500m\n",
    "    mesh = population_to_mesh(population, step=0.005, col = 'population', fill_missing='centroid')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "b925263c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "46153"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(mesh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "6a276040",
   "metadata": {},
   "outputs": [],
   "source": [
    "mesh.to_file(output_folder + 'population_mesh.geojson',driver='GeoJSON')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfbafa49",
   "metadata": {},
   "source": [
    "# catchment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "aae1e149",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find TC nodes to mesh distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "ac854281",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_dist = max(max(catchment_radius.values()),default_catchment_radius)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "ffc2ca97",
   "metadata": {},
   "outputs": [],
   "source": [
    "if rnodes_file_provided:\n",
    "    print('using road_nodes')\n",
    "    node_dist = get_routing_distances(nodes, rnodes, rlinks, mesh, 'length', max_dist)\n",
    "else:\n",
    "    node_dist = get_acf_distances(nodes, mesh, crs, max_dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "496d9cfe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3480e8ac",
   "metadata": {},
   "source": [
    "# metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "6ed69263",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num route_id: 495\n",
      "num route_type: 3\n"
     ]
    }
   ],
   "source": [
    "print('num route_id:',len(links['route_id'].unique()))\n",
    "print('num route_type:',len(links['route_type'].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "dc317930",
   "metadata": {},
   "outputs": [],
   "source": [
    "#init results dfs\n",
    "df_route_id = pd.DataFrame(index=links['route_id'].unique())\n",
    "df_route_id.index.name='route_id'\n",
    "\n",
    "df_route_type = pd.DataFrame(index=links['route_type'].unique())\n",
    "df_route_type.index.name='route_type'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "45bc4d79",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_catchment(col='route_id'):\n",
    "    #get all nodes with col filter\n",
    "    link = links.groupby(col)[['a','b','route_type']].agg({'a':set,'b':set,'route_type':'first'})\n",
    "    link['node'] = link.apply(lambda row: row['a'].union(row['b']), axis=1)\n",
    "    link = link.drop(columns=['a','b'])\n",
    "    # add catchment radius for the route_type\n",
    "    link['catchment_radius'] = link['route_type'].apply(lambda x: catchment_radius.get(x,default_catchment_radius))\n",
    "\n",
    "    col_exist = col == 'route_type' # cannot explode if index == route_type (a column)\n",
    "    link = link.explode('node').reset_index(drop=col_exist)\n",
    "    link = node_dist.merge(link, left_on='node_index', right_on='node')\n",
    "    #filter by distance\n",
    "    link = link[link['distances'] <= link['catchment_radius']]\n",
    "    #drop duplicated mesh nodes (we count only one time)\n",
    "    link = link.drop_duplicates(subset=['mesh_index',col],keep='first')\n",
    "\n",
    "    return link.groupby(col)['population'].sum().to_dict()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "e18e10d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18178349.199833468\n"
     ]
    }
   ],
   "source": [
    "res = get_catchment('route_id')\n",
    "\n",
    "df_route_id['catchment'] = res\n",
    "print(sum([item for key,item in res.items()]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "06d98402",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4418728.561923779\n"
     ]
    }
   ],
   "source": [
    "res = get_catchment('route_type')\n",
    "\n",
    "df_route_type['catchment'] = res\n",
    "print(sum([item for key,item in res.items()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d35b71b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "5ff69b2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    links['network']=True\n",
    "    res=[]\n",
    "    dists = [0,1,10,20,50,100,250,500,800,1000]\n",
    "    for dist in dists:\n",
    "        col='network'\n",
    "        link = links.groupby(col)[['a','b','route_type']].agg({'a':set,'b':set,'route_type':'first'})\n",
    "        link['node'] = link.apply(lambda row: row['a'].union(row['b']), axis=1)\n",
    "        link = link.drop(columns=['a','b'])\n",
    "        # add catchment radius for the route_type\n",
    "        link['catchment_radius'] =dist\n",
    "\n",
    "        col_exist = col == 'route_type' # cannot explode if index == route_type (a column)\n",
    "        link = link.explode('node').reset_index(drop=col_exist)\n",
    "        link = node_dist.merge(link, left_on='node_index', right_on='node')\n",
    "        #filter by distance\n",
    "        link = link[link['distances'] <= link['catchment_radius']]\n",
    "        #drop duplicated mesh nodes (we count only one time)\n",
    "        link = link.drop_duplicates(subset=['mesh_index',col],keep='first')\n",
    "        volume = link['population'].sum()\n",
    "        res.append(volume)\n",
    "    plt.plot(dists,res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1c2a4e2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "de5d12fd",
   "metadata": {},
   "source": [
    "# frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "3f251696",
   "metadata": {},
   "outputs": [],
   "source": [
    "links['frequency'] = 1/links['headway']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "3829349f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1179.0267365679042\n"
     ]
    }
   ],
   "source": [
    "res = (links.groupby('route_id')['frequency'].agg(np.nanmean)*3600).to_dict()\n",
    "\n",
    "df_route_id['frequency (veh/hours)'] = res\n",
    "print(np.nansum([item for key,item in res.items()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "e3035415",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20.888085455759082\n"
     ]
    }
   ],
   "source": [
    "res = (links.groupby('route_type')['frequency'].agg(np.nanmean)*3600).to_dict()\n",
    "\n",
    "df_route_type['frequency (veh/hours)'] = res\n",
    "print(sum([item for key,item in res.items()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "b1017844",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "975.8001839437252\n"
     ]
    }
   ],
   "source": [
    "link = (links.groupby(['route_id','trip_id'])[['frequency']].agg(np.nanmean)*3600)\n",
    "res = link.reset_index().set_index('route_id')['frequency'].to_dict()\n",
    "print(np.nansum([item for key,item in res.items()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "99bf3c5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16.018955285524857\n"
     ]
    }
   ],
   "source": [
    "link = (links.groupby(['route_type','trip_id'])[['frequency']].agg(np.nanmean)*3600)\n",
    "res = link.reset_index().set_index('route_type')['frequency'].to_dict()\n",
    "print(np.nansum([item for key,item in res.items()]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06a9b56e",
   "metadata": {},
   "source": [
    "# operational Fleet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "d73f2028",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_fleet(col='route_id'):\n",
    "    link = links.groupby([col,'trip_id'])[['time','frequency']].agg({'time':np.nansum,'frequency':np.nanmean})\n",
    "    link['fleet'] = np.ceil(link['frequency'] * link['time'])\n",
    "    return link.reset_index().groupby(col)['fleet'].agg(np.nansum).to_dict()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "88e247d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1623.0\n"
     ]
    }
   ],
   "source": [
    "res = get_fleet('route_id')\n",
    "\n",
    "df_route_id['fleet'] = res\n",
    "print(sum([item for key,item in res.items()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "0dc17813",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1623.0\n"
     ]
    }
   ],
   "source": [
    "res = get_fleet('route_type')\n",
    "\n",
    "df_route_type['fleet'] = res\n",
    "print(sum([item for key,item in res.items()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df7743de",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b0a16eb3",
   "metadata": {},
   "source": [
    "# Line Length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "5e08e66e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_length(col='route_id'):\n",
    "    link = links.groupby([col,'trip_id'])[[length_col]].agg(np.nansum)\n",
    "    return link.reset_index().groupby(col)[length_col].agg(np.nansum).to_dict()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "bfa2e75e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rpeparation. if legnth is NaN, or if shsape dist travel exist.\n",
    "\n",
    "length_col = None\n",
    "if 'shape_dist_traveled' in links.columns and length_col == None:\n",
    "    if len(links[links['shape_dist_traveled'].isnull()])==0:\n",
    "        length_col = 'shape_dist_traveled'\n",
    "if 'length' in links.columns and length_col == None:\n",
    "    if len(links[links['length'].isnull()])==0:\n",
    "        length_col = 'length'\n",
    "\n",
    "if length_col == None:\n",
    "    print('create length from geometry')\n",
    "    links['length'] = links.to_crs(crs).length\n",
    "    length_col = 'length'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa1f7686",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "31db0758",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15622877.232517038\n"
     ]
    }
   ],
   "source": [
    "res = get_length('route_id')\n",
    "\n",
    "df_route_id['length (m)'] = res\n",
    "print(sum([item for key,item in res.items()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "39826011",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15622877.232517032\n"
     ]
    }
   ],
   "source": [
    "res = get_length('route_type')\n",
    "\n",
    "df_route_type['length (m)'] = res\n",
    "print(sum([item for key,item in res.items()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7f4f68b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "638013ee",
   "metadata": {},
   "source": [
    "# Number of station per line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "6bf1bd3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# o-->o-->o-->o and  o<--o<--o<--o\n",
    "# est-ce que j'ai 8 ou 4 stations ?\n",
    "# j'ai 4 stations par trip et 4 stations par route (si c'est les memes).\n",
    "# comment savoir si cest les memes. clustering?\n",
    "# pour linstant. on prend tous les noeds unique par route_id ou route_type (col='route_id', route_id)\n",
    "def get_num_station(col='route_id'):\n",
    "    link = links.groupby(col)[['a','b']].agg({'a':set,'b':set})\n",
    "    link['node_len'] = link.apply(lambda row: len(row['a'].union(row['b'])), axis=1)\n",
    "    return link['node_len'].to_dict()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "6f27ad9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41517\n"
     ]
    }
   ],
   "source": [
    "res = get_num_station('route_id')\n",
    "\n",
    "df_route_id['num station'] = res\n",
    "print(sum([item for key,item in res.items()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "ea89801d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32871\n"
     ]
    }
   ],
   "source": [
    "res = get_num_station('route_type')\n",
    "\n",
    "df_route_type['num station'] = res\n",
    "print(sum([item for key,item in res.items()]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f98a64c",
   "metadata": {},
   "source": [
    "# Vehicle revenue KM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "c70257d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_veh_kmh(col='route_id'):\n",
    "    link = links.groupby([col,'trip_id'])[[length_col,'frequency']].agg({length_col:np.nansum,'frequency':np.nanmean})\n",
    "    link['veh_km/h'] = np.ceil(link['frequency'] * link[length_col]) * 3600/1000 #to km/H\n",
    "    return link.reset_index().groupby(col)['veh_km/h'].agg(np.nansum).to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "6b53c067",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30308.399999999998\n"
     ]
    }
   ],
   "source": [
    "res = get_veh_kmh('route_id')\n",
    "\n",
    "df_route_id['veh.km/h'] = res\n",
    "print(sum([item for key,item in res.items()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "3bc451d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30308.399999999998\n"
     ]
    }
   ],
   "source": [
    "res = get_veh_kmh('route_type')\n",
    "\n",
    "df_route_type['veh.km/h'] = res\n",
    "print(sum([item for key,item in res.items()]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab77743f",
   "metadata": {},
   "source": [
    "# Round trip time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "fd818c88",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_round_trip_time(col='route_id'):\n",
    "    link = links.groupby([col,'trip_id'])[['time']].agg(np.nansum)\n",
    "    return link.reset_index().groupby(col)['time'].agg(np.nansum).to_dict()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "869ea67e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2035849.0\n"
     ]
    }
   ],
   "source": [
    "res = get_round_trip_time('route_id')\n",
    "\n",
    "df_route_id['round trip time (s)'] = res\n",
    "print(sum([item for key,item in res.items()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ac6bf88",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "984694c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>catchment</th>\n",
       "      <th>frequency (veh/hours)</th>\n",
       "      <th>fleet</th>\n",
       "      <th>length (m)</th>\n",
       "      <th>num station</th>\n",
       "      <th>veh.km/h</th>\n",
       "      <th>round trip time (s)</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>route_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>OMITSJU_350</th>\n",
       "      <td>2789.790909</td>\n",
       "      <td>2.514685</td>\n",
       "      <td>2.0</td>\n",
       "      <td>42180.0</td>\n",
       "      <td>4</td>\n",
       "      <td>111.6</td>\n",
       "      <td>2460.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>OMITSJU_330</th>\n",
       "      <td>7231.541126</td>\n",
       "      <td>0.670391</td>\n",
       "      <td>1.0</td>\n",
       "      <td>28995.0</td>\n",
       "      <td>8</td>\n",
       "      <td>21.6</td>\n",
       "      <td>2760.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>OMITSJU_325</th>\n",
       "      <td>7592.053030</td>\n",
       "      <td>1.005587</td>\n",
       "      <td>1.0</td>\n",
       "      <td>53450.0</td>\n",
       "      <td>10</td>\n",
       "      <td>28.8</td>\n",
       "      <td>5040.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>OMITSJU_600</th>\n",
       "      <td>6968.436364</td>\n",
       "      <td>3.128492</td>\n",
       "      <td>4.0</td>\n",
       "      <td>71398.0</td>\n",
       "      <td>5</td>\n",
       "      <td>194.4</td>\n",
       "      <td>4500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>OMITSJU_3</th>\n",
       "      <td>12851.074516</td>\n",
       "      <td>2.682563</td>\n",
       "      <td>2.0</td>\n",
       "      <td>9871.0</td>\n",
       "      <td>42</td>\n",
       "      <td>28.8</td>\n",
       "      <td>1440.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CITROUS_38</th>\n",
       "      <td>9115.812500</td>\n",
       "      <td>2.011173</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7505.0</td>\n",
       "      <td>28</td>\n",
       "      <td>18.0</td>\n",
       "      <td>960.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CITROUS_40</th>\n",
       "      <td>14626.619318</td>\n",
       "      <td>2.682563</td>\n",
       "      <td>1.0</td>\n",
       "      <td>9467.0</td>\n",
       "      <td>37</td>\n",
       "      <td>28.8</td>\n",
       "      <td>1320.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CITROUS_43</th>\n",
       "      <td>21376.767045</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>16744.0</td>\n",
       "      <td>61</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2280.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CITROUS_210</th>\n",
       "      <td>7895.450000</td>\n",
       "      <td>2.011173</td>\n",
       "      <td>3.0</td>\n",
       "      <td>48139.0</td>\n",
       "      <td>5</td>\n",
       "      <td>97.2</td>\n",
       "      <td>3600.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CITROUS_115</th>\n",
       "      <td>21028.099376</td>\n",
       "      <td>2.011173</td>\n",
       "      <td>2.0</td>\n",
       "      <td>27131.0</td>\n",
       "      <td>22</td>\n",
       "      <td>57.6</td>\n",
       "      <td>2940.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>495 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                catchment  frequency (veh/hours)  fleet  length (m)  \\\n",
       "route_id                                                              \n",
       "OMITSJU_350   2789.790909               2.514685    2.0     42180.0   \n",
       "OMITSJU_330   7231.541126               0.670391    1.0     28995.0   \n",
       "OMITSJU_325   7592.053030               1.005587    1.0     53450.0   \n",
       "OMITSJU_600   6968.436364               3.128492    4.0     71398.0   \n",
       "OMITSJU_3    12851.074516               2.682563    2.0      9871.0   \n",
       "...                   ...                    ...    ...         ...   \n",
       "CITROUS_38    9115.812500               2.011173    1.0      7505.0   \n",
       "CITROUS_40   14626.619318               2.682563    1.0      9467.0   \n",
       "CITROUS_43   21376.767045                    NaN    0.0     16744.0   \n",
       "CITROUS_210   7895.450000               2.011173    3.0     48139.0   \n",
       "CITROUS_115  21028.099376               2.011173    2.0     27131.0   \n",
       "\n",
       "             num station  veh.km/h  round trip time (s)  \n",
       "route_id                                                 \n",
       "OMITSJU_350            4     111.6               2460.0  \n",
       "OMITSJU_330            8      21.6               2760.0  \n",
       "OMITSJU_325           10      28.8               5040.0  \n",
       "OMITSJU_600            5     194.4               4500.0  \n",
       "OMITSJU_3             42      28.8               1440.0  \n",
       "...                  ...       ...                  ...  \n",
       "CITROUS_38            28      18.0                960.0  \n",
       "CITROUS_40            37      28.8               1320.0  \n",
       "CITROUS_43            61       0.0               2280.0  \n",
       "CITROUS_210            5      97.2               3600.0  \n",
       "CITROUS_115           22      57.6               2940.0  \n",
       "\n",
       "[495 rows x 7 columns]"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_route_id.to_csv(output_folder+'route_id_metrics.csv')\n",
    "df_route_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "c6a5c283",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>catchment</th>\n",
       "      <th>frequency (veh/hours)</th>\n",
       "      <th>fleet</th>\n",
       "      <th>length (m)</th>\n",
       "      <th>num station</th>\n",
       "      <th>veh.km/h</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>route_type</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>bus</th>\n",
       "      <td>3.245550e+06</td>\n",
       "      <td>2.508795</td>\n",
       "      <td>1545.0</td>\n",
       "      <td>1.488548e+07</td>\n",
       "      <td>32527</td>\n",
       "      <td>27802.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>subway</th>\n",
       "      <td>6.592749e+05</td>\n",
       "      <td>16.763005</td>\n",
       "      <td>63.0</td>\n",
       "      <td>1.473132e+05</td>\n",
       "      <td>174</td>\n",
       "      <td>2062.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rail</th>\n",
       "      <td>5.139035e+05</td>\n",
       "      <td>1.616285</td>\n",
       "      <td>15.0</td>\n",
       "      <td>5.900830e+05</td>\n",
       "      <td>170</td>\n",
       "      <td>442.8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               catchment  frequency (veh/hours)   fleet    length (m)  \\\n",
       "route_type                                                              \n",
       "bus         3.245550e+06               2.508795  1545.0  1.488548e+07   \n",
       "subway      6.592749e+05              16.763005    63.0  1.473132e+05   \n",
       "rail        5.139035e+05               1.616285    15.0  5.900830e+05   \n",
       "\n",
       "            num station  veh.km/h  \n",
       "route_type                         \n",
       "bus               32527   27802.8  \n",
       "subway              174    2062.8  \n",
       "rail                170     442.8  "
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_route_type.to_csv(output_folder+'route_type_metrics.csv')\n",
    "df_route_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42ad4b64",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9dc7238",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba0c4f26",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4924d2a1",
   "metadata": {},
   "source": [
    "# geomatic outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "10166c45",
   "metadata": {},
   "outputs": [],
   "source": [
    "#using get catchment. get the catchment radius of each node (get larger one if used by many mode.)\n",
    "link = links.groupby('route_type')[['a','b','route_type']].agg({'a':set,'b':set,'route_type':'first'})\n",
    "link['node'] = link.apply(lambda row: row['a'].union(row['b']), axis=1)\n",
    "link = link.drop(columns=['a','b'])\n",
    "# add catchment radius for the route_type\n",
    "link['catchment_radius'] = link['route_type'].apply(lambda x: catchment_radius.get(x,default_catchment_radius))\n",
    "link = link.explode('node').reset_index(drop=True)\n",
    "link = link.sort_values('catchment_radius',ascending=False).drop_duplicates('node',keep='first')\n",
    "link = node_dist.merge(link, left_on='node_index', right_on='node')\n",
    "link = link[link['distances'] <= link['catchment_radius']]\n",
    "\n",
    "temp_dict = link.groupby('node_index')['population'].sum().to_dict()\n",
    "nodes['catchment'] = nodes.index.map(temp_dict.get)\n",
    "\n",
    "temp_dict = link.groupby('node_index')['catchment_radius'].agg('first').to_dict() \n",
    "nodes['catchment_radius'] = nodes.index.map(temp_dict.get)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "db8164da",
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes.to_file(output_folder+'nodes.geojson',driver='GeoJSON')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc77392d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "11dfe2fb",
   "metadata": {},
   "source": [
    "# test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9551a899",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02cf7acf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
